[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "AI and Mental Health: Can Machines Help Us Feel Better?\n\n\n\n\n\n\nLLMs\n\n\nNLPs\n\n\nlogic\n\n\n\nThis blog explores the potential of AI-powered mental health tools, evaluating their ability to provide emotional support, track moods, and offer coping strategies, while comparing their strengths and limitations to human therapy.\n\n\n\n\n\nFeb 11, 2025\n\n\nNora Elliott\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Posts Folder/Post3.html",
    "href": "posts/Posts Folder/Post3.html",
    "title": "AI and Mental Health: Can Machines Help Us Feel Better?",
    "section": "",
    "text": "We’ve seen AI take on everything from self-driving cars to writing essays, but can it truly understand human emotions and provide meaningful mental health support? To find out, I explored the world of AI-powered mental health apps to see what they offer, how effective they are, and where they fall short.\nI tested several AI-driven mental health tools, including chatbots like Woebot, Wysa, and Replika, and rated their ability to provide emotional support, practical coping strategies, and a sense of connection. Here’s what I discovered."
  },
  {
    "objectID": "posts/Posts Folder/Post3.html#ai-and-guided-mental-health-exercises-do-they-work",
    "href": "posts/Posts Folder/Post3.html#ai-and-guided-mental-health-exercises-do-they-work",
    "title": "AI and Mental Health: Can Machines Help Us Feel Better?",
    "section": "AI and Guided Mental Health Exercises: Do They Work?",
    "text": "AI and Guided Mental Health Exercises: Do They Work?\nBeyond chatbots, AI-powered apps like Youper, Sanvello, and MindDoc provide structured mental health exercises to help manage anxiety and depression.\n\nYouper\nUses AI to help track moods and suggest mindfulness techniques. Rating: 7/10 #### Sanvello Focuses on evidence-based mental health strategies, with guided meditations and journaling prompts. Rating: 8/10 #### MindDoc Offers AI-assisted mood tracking and journaling for people struggling with anxiety and depression. Rating: 6/10\n\n\nWhere do AI Mental Health Apps Succeed\n\nthey help users track emotional patterns over time, which can be valuable for understanding mental health triggers\nthey provide structured coping strageies, such as mindfulness and guided breathing exercises\nthey offer privacy and accessibility, making mental health tools available to anyone, anywhere\n\n\n\nWhere do AI Mental Health Apps Struggle\n\nAI can provide generalized advice, but it lacks the nuance of a human therapist\nThey rely heavily on self guided effort, if you don’t engage with the exercises, they won’t be effective\nSevere mental health conditions require human intervention, which AI can’t replace"
  },
  {
    "objectID": "posts/Posts Folder/Post3.html#what-makes-ai-mental-health-tools-different-from-human-therapy",
    "href": "posts/Posts Folder/Post3.html#what-makes-ai-mental-health-tools-different-from-human-therapy",
    "title": "AI and Mental Health: Can Machines Help Us Feel Better?",
    "section": "What Makes AI Mental Health Tools Different from Human Therapy?",
    "text": "What Makes AI Mental Health Tools Different from Human Therapy?\n\n\n\ndownload.jpg\n\n\nThe biggest difference between AI driven mental health tools and human therapists is depth and connection\nA therapist doesn’t just provide a list of coping mechanisms, they also listen and provide validation based on clients emotions, history and experiences. While AI follows a structured approach based on recognized patterns. It can suggest deep-breathing exercises when you say you’re anxious, but it won’t ask about your childhood or recognize patterns in the way a human therapist would.\nAI doesn’t understand emotional nuance. It can recognize words like “sad” or “stressed” but won’t fully grasp sarcasm, deep emotional pain, or the kind of comfort humans offer through shared experiences. A therapist might read body language, notice a change in tone, or offer a moment of silence—things that AI can’t replicate.\nThat said, AI still has a role to play. It can provide instant access to support when needed and help users build healthy mental habits. It may not replace therapists, but it can complement traditional therapy by offering tools for managing mental health between sessions."
  },
  {
    "objectID": "posts/Posts Folder/Post3.html#where-does-ais-mental-health-support-come-from",
    "href": "posts/Posts Folder/Post3.html#where-does-ais-mental-health-support-come-from",
    "title": "AI and Mental Health: Can Machines Help Us Feel Better?",
    "section": "Where Does AI’s Mental Health Support Come From?",
    "text": "Where Does AI’s Mental Health Support Come From?\n\n\n\n01-risk-of-depressive-disorder.webp\n\n\nAI mental health tools are trained on thousands of therapy transcripts, psychology research papers, and wellness articles. They recognize patterns in human emotions and provide respinses based on CBT principles, mindfullness tecnhniques, and self help strategies. However, they do not possess genuine human empathy or understanding, they mimic supportive behavior rather than truly feeling it\nAI learns from data, not experiences, which means it lacks the depth of human insight. But that doesn’t mean it can’t be useful, it just means it should be viewed as a tool rather than a replacement for real human care."
  },
  {
    "objectID": "posts/Posts Folder/Post3.html#what-surprised-me-about-ai-mental-health-apps",
    "href": "posts/Posts Folder/Post3.html#what-surprised-me-about-ai-mental-health-apps",
    "title": "AI and Mental Health: Can Machines Help Us Feel Better?",
    "section": "What surprised me about AI mental health apps?",
    "text": "What surprised me about AI mental health apps?\nDespite their limitations, AI mental health tools were more helpful than I expected in certain areas\n\nAI is great at mood tracking, apps like youper and minddoc provided useful insights into emotional patterns, which coukd ve valuable for self reflection\nAI responses felt more human than I expected, some chatbot interactions felt genuinely warm and engaging\nInstant support is a big plus, AI is available 24/7, which is a major advantage for people who need immediate comfort"
  },
  {
    "objectID": "posts/Posts Folder/Post3.html#can-ai-really-help-with-mental-health",
    "href": "posts/Posts Folder/Post3.html#can-ai-really-help-with-mental-health",
    "title": "AI and Mental Health: Can Machines Help Us Feel Better?",
    "section": "Can AI Really Help with Mental Health?",
    "text": "Can AI Really Help with Mental Health?\n\n\n\npsychology-therapy-isometric-illustration_1284-58097.avif\n\n\n\nYes - but with limitations\nAI mental health tools can provide valuable support, offer coping strategies, and help users track emotions, but they don’t replace the deoth, understanding, and personal connection of humab therapists.\nFor mild anxiety, stress management, or general emotional well-being, AI-driven apps can be a great starting point. But for deep emotional struggles, complex mental health conditions, or anyone in crisis, human interaction is irreplacable.\nWould I trust AI as my sole therapist? Not yet. But am I impressed by how much it can assist in daily mental health maintenance? Absolutely.\nAI in mental health isn’t about replacing human connection—it’s about enhancing accesibility and providing support when we need it most. And in that sense, AI might help us feel a little better."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  }
]